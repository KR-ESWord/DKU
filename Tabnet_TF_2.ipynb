{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load (iris dataset) & Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 125\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ds):\n",
    "    features = tf.unstack(ds['features'])\n",
    "    labels = ds['label']\n",
    "\n",
    "    x = dict(zip(col_names, features))\n",
    "    y = tf.one_hot(labels, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "ds_full = tfds.load(name=\"iris\", split=tfds.Split.TRAIN)\n",
    "ds_full = ds_full.shuffle(150, seed=0)\n",
    "\n",
    "ds_train = ds_full.take(train_size)\n",
    "ds_train = ds_train.map(transform)\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "\n",
    "ds_test = ds_full.skip(train_size)\n",
    "ds_test = ds_test.map(transform)\n",
    "ds_test = ds_test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/282122103.py:3: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/282122103.py:3: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "for col_name in col_names:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(col_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_keras_custom_object(cls):\n",
    "    tf.keras.utils.get_custom_objects()[cls.__name__] = cls\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glu(x, n_units=None):\n",
    "    \"\"\"Generalized linear unit nonlinear activation.\"\"\"\n",
    "    if n_units is None:\n",
    "        n_units = tf.shape(x)[-1] // 2\n",
    "\n",
    "    return x[..., :n_units] * tf.nn.sigmoid(x[..., n_units:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sparsemax(logits, axis):\n",
    "    \"\"\"Sparsemax activation function [1].\n",
    "    For each batch `i` and class `j` we have\n",
    "      $$sparsemax[i, j] = max(logits[i, j] - tau(logits[i, :]), 0)$$\n",
    "    [1]: https://arxiv.org/abs/1602.02068\n",
    "    Args:\n",
    "        logits: Input tensor.\n",
    "        axis: Integer, axis along which the sparsemax operation is applied.\n",
    "    Returns:\n",
    "        Tensor, output of sparsemax transformation. Has the same type and\n",
    "        shape as `logits`.\n",
    "    Raises:\n",
    "        ValueError: In case `dim(logits) == 1`.\n",
    "    \"\"\"\n",
    "    logits = tf.convert_to_tensor(logits, name=\"logits\")\n",
    "\n",
    "    # We need its original shape for shape inference.\n",
    "    shape = logits.get_shape()\n",
    "    rank = shape.rank\n",
    "    is_last_axis = (axis == -1) or (axis == rank - 1)\n",
    "\n",
    "    if is_last_axis:\n",
    "        output = _compute_2d_sparsemax(logits)\n",
    "        output.set_shape(shape)\n",
    "        return output\n",
    "\n",
    "    # If dim is not the last dimension, we have to do a transpose so that we can\n",
    "    # still perform softmax on its last dimension.\n",
    "\n",
    "    # Swap logits' dimension of dim and its last dimension.\n",
    "    rank_op = tf.rank(logits)\n",
    "    axis_norm = axis % rank\n",
    "    logits = _swap_axis(logits, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Do the actual softmax on its last dimension.\n",
    "    output = _compute_2d_sparsemax(logits)\n",
    "    output = _swap_axis(output, axis_norm, tf.math.subtract(rank_op, 1))\n",
    "\n",
    "    # Make shape inference work since transpose may erase its static shape.\n",
    "    output.set_shape(shape)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _swap_axis(logits, dim_index, last_index, **kwargs):\n",
    "    return tf.transpose(\n",
    "        logits,\n",
    "        tf.concat(\n",
    "            [\n",
    "                tf.range(dim_index),\n",
    "                [last_index],\n",
    "                tf.range(dim_index + 1, last_index),\n",
    "                [dim_index],\n",
    "            ],\n",
    "            0,\n",
    "        ),\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_2d_sparsemax(logits):\n",
    "    \"\"\"Performs the sparsemax operation when axis=-1.\"\"\"\n",
    "    shape_op = tf.shape(logits)\n",
    "    obs = tf.math.reduce_prod(shape_op[:-1])\n",
    "    dims = shape_op[-1]\n",
    "\n",
    "    # In the paper, they call the logits z.\n",
    "    # The mean(logits) can be substracted from logits to make the algorithm\n",
    "    # more numerically stable. the instability in this algorithm comes mostly\n",
    "    # from the z_cumsum. Substacting the mean will cause z_cumsum to be close\n",
    "    # to zero. However, in practise the numerical instability issues are very\n",
    "    # minor and substacting the mean causes extra issues with inf and nan\n",
    "    # input.\n",
    "    # Reshape to [obs, dims] as it is almost free and means the remanining\n",
    "    # code doesn't need to worry about the rank.\n",
    "    z = tf.reshape(logits, [obs, dims])\n",
    "\n",
    "    # sort z\n",
    "    z_sorted, _ = tf.nn.top_k(z, k=dims)\n",
    "\n",
    "    # calculate k(z)\n",
    "    z_cumsum = tf.math.cumsum(z_sorted, axis=-1)\n",
    "    k = tf.range(1, tf.cast(dims, logits.dtype) + 1, dtype=logits.dtype)\n",
    "    z_check = 1 + k * z_sorted > z_cumsum\n",
    "    # because the z_check vector is always [1,1,...1,0,0,...0] finding the\n",
    "    # (index + 1) of the last `1` is the same as just summing the number of 1.\n",
    "    k_z = tf.math.reduce_sum(tf.cast(z_check, tf.int32), axis=-1)\n",
    "\n",
    "    # calculate tau(z)\n",
    "    # If there are inf values or all values are -inf, the k_z will be zero,\n",
    "    # this is mathematically invalid and will also cause the gather_nd to fail.\n",
    "    # Prevent this issue for now by setting k_z = 1 if k_z = 0, this is then\n",
    "    # fixed later (see p_safe) by returning p = nan. This results in the same\n",
    "    # behavior as softmax.\n",
    "    k_z_safe = tf.math.maximum(k_z, 1)\n",
    "    indices = tf.stack([tf.range(0, obs), tf.reshape(k_z_safe, [-1]) - 1], axis=1)\n",
    "    tau_sum = tf.gather_nd(z_cumsum, indices)\n",
    "    tau_z = (tau_sum - 1) / tf.cast(k_z, logits.dtype)\n",
    "\n",
    "    # calculate p\n",
    "    p = tf.math.maximum(tf.cast(0, logits.dtype), z - tf.expand_dims(tau_z, -1))\n",
    "    # If k_z = 0 or if z = nan, then the input is invalid\n",
    "    p_safe = tf.where(\n",
    "        tf.expand_dims(\n",
    "            tf.math.logical_or(tf.math.equal(k_z, 0), tf.math.is_nan(z_cumsum[:, -1])),\n",
    "            axis=-1,\n",
    "        ),\n",
    "        tf.fill([obs, dims], tf.cast(float(\"nan\"), logits.dtype)),\n",
    "        p,\n",
    "    )\n",
    "\n",
    "    # Reshape back to original size\n",
    "    p_safe = tf.reshape(p_safe, shape_op)\n",
    "    return p_safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ghost Batch Normalization class\n",
    "class GhostBatchNormalization(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self, virtual_divider: int = 1, momentum: float = 0.9, epsilon: float = 1e-5\n",
    "    ):\n",
    "        super(GhostBatchNormalization, self).__init__()\n",
    "        self.virtual_divider = virtual_divider\n",
    "        self.bn = BatchNormInferenceWeighting(momentum=momentum)\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        if training:\n",
    "            chunks = tf.split(x, self.virtual_divider)\n",
    "            x = [self.bn(x, training=True) for x in chunks]\n",
    "            return tf.concat(x, 0)\n",
    "        return self.bn(x, training=False, alpha=alpha)\n",
    "\n",
    "    @property\n",
    "    def moving_mean(self):\n",
    "        return self.bn.moving_mean\n",
    "\n",
    "    @property\n",
    "    def moving_variance(self):\n",
    "        return self.bn.moving_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormInferenceWeighting(tf.keras.layers.Layer):\n",
    "    def __init__(self, momentum: float = 0.9, epsilon: float = None):\n",
    "        super(BatchNormInferenceWeighting, self).__init__()\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = tf.keras.backend.epsilon() if epsilon is None else epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        channels = input_shape[-1]\n",
    "\n",
    "        self.gamma = tf.Variable(\n",
    "            initial_value=tf.ones((channels,), tf.float32), trainable=True,\n",
    "        )\n",
    "        self.beta = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=True,\n",
    "        )\n",
    "\n",
    "        self.moving_mean = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,\n",
    "        )\n",
    "        self.moving_mean_of_squares = tf.Variable(\n",
    "            initial_value=tf.zeros((channels,), tf.float32), trainable=False,\n",
    "        )\n",
    "\n",
    "    def __update_moving(self, var, value):\n",
    "        var.assign(var * self.momentum + (1 - self.momentum) * value)\n",
    "\n",
    "    def __apply_normalization(self, x, mean, variance):\n",
    "        return self.gamma * (x - mean) / tf.sqrt(variance + self.epsilon) + self.beta\n",
    "\n",
    "    def call(self, x, training: bool = None, alpha: float = 0.0):\n",
    "        mean = tf.reduce_mean(x, axis=0)\n",
    "        mean_of_squares = tf.reduce_mean(tf.pow(x, 2), axis=0)\n",
    "\n",
    "        if training:\n",
    "            # update moving stats\n",
    "            self.__update_moving(self.moving_mean, mean)\n",
    "            self.__update_moving(self.moving_mean_of_squares, mean_of_squares)\n",
    "\n",
    "            variance = mean_of_squares - tf.pow(mean, 2)\n",
    "            x = self.__apply_normalization(x, mean, variance)\n",
    "        else:\n",
    "            mean = alpha * mean + (1 - alpha) * self.moving_mean\n",
    "            variance = (\n",
    "                alpha * mean_of_squares + (1 - alpha) * self.moving_mean_of_squares\n",
    "            ) - tf.pow(mean, 2)\n",
    "            x = self.__apply_normalization(x, mean, variance)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"Group normalization layer.\n",
    "    Group Normalization divides the channels into groups and computes\n",
    "    within each group the mean and variance for normalization.\n",
    "    Empirically, its accuracy is more stable than batch norm in a wide\n",
    "    range of small batch sizes, if learning rate is adjusted linearly\n",
    "    with batch sizes.\n",
    "    Relation to Layer Normalization:\n",
    "    If the number of groups is set to 1, then this operation becomes identical\n",
    "    to Layer Normalization.\n",
    "    Relation to Instance Normalization:\n",
    "    If the number of groups is set to the\n",
    "    input dimension (number of groups is equal\n",
    "    to number of channels), then this operation becomes\n",
    "    identical to Instance Normalization.\n",
    "    Arguments\n",
    "        groups: Integer, the number of groups for Group Normalization.\n",
    "            Can be in the range [1, N] where N is the input dimension.\n",
    "            The input dimension must be divisible by the number of groups.\n",
    "        axis: Integer, the axis that should be normalized.\n",
    "        epsilon: Small float added to variance to avoid dividing by zero.\n",
    "        center: If True, add offset of `beta` to normalized tensor.\n",
    "            If False, `beta` is ignored.\n",
    "        scale: If True, multiply by `gamma`.\n",
    "            If False, `gamma` is not used.\n",
    "        beta_initializer: Initializer for the beta weight.\n",
    "        gamma_initializer: Initializer for the gamma weight.\n",
    "        beta_regularizer: Optional regularizer for the beta weight.\n",
    "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
    "        beta_constraint: Optional constraint for the beta weight.\n",
    "        gamma_constraint: Optional constraint for the gamma weight.\n",
    "    Input shape\n",
    "        Arbitrary. Use the keyword argument `input_shape`\n",
    "        (tuple of integers, does not include the samples axis)\n",
    "        when using this layer as the first layer in a model.\n",
    "    Output shape\n",
    "        Same shape as input.\n",
    "    References\n",
    "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            groups: int = 2,\n",
    "            axis: int = -1,\n",
    "            epsilon: float = 1e-3,\n",
    "            center: bool = True,\n",
    "            scale: bool = True,\n",
    "            beta_initializer=\"zeros\",\n",
    "            gamma_initializer=\"ones\",\n",
    "            beta_regularizer=None,\n",
    "            gamma_regularizer=None,\n",
    "            beta_constraint=None,\n",
    "            gamma_constraint=None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.groups = groups\n",
    "        self.axis = axis\n",
    "        self.epsilon = epsilon\n",
    "        self.center = center\n",
    "        self.scale = scale\n",
    "        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n",
    "        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n",
    "        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n",
    "        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n",
    "        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n",
    "        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n",
    "        self._check_axis()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        self._check_if_input_shape_is_none(input_shape)\n",
    "        self._set_number_of_groups_for_instance_norm(input_shape)\n",
    "        self._check_size_of_dimensions(input_shape)\n",
    "        self._create_input_spec(input_shape)\n",
    "\n",
    "        self._add_gamma_weight(input_shape)\n",
    "        self._add_beta_weight(input_shape)\n",
    "        self.built = True\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Training=none is just for compat with batchnorm signature call\n",
    "        input_shape = tf.keras.backend.int_shape(inputs)\n",
    "        tensor_input_shape = tf.shape(inputs)\n",
    "\n",
    "        reshaped_inputs, group_shape = self._reshape_into_groups(\n",
    "            inputs, input_shape, tensor_input_shape\n",
    "        )\n",
    "\n",
    "        normalized_inputs = self._apply_normalization(reshaped_inputs, input_shape)\n",
    "\n",
    "        outputs = tf.reshape(normalized_inputs, tensor_input_shape)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"groups\": self.groups,\n",
    "            \"axis\": self.axis,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"center\": self.center,\n",
    "            \"scale\": self.scale,\n",
    "            \"beta_initializer\": tf.keras.initializers.serialize(self.beta_initializer),\n",
    "            \"gamma_initializer\": tf.keras.initializers.serialize(\n",
    "                self.gamma_initializer\n",
    "            ),\n",
    "            \"beta_regularizer\": tf.keras.regularizers.serialize(self.beta_regularizer),\n",
    "            \"gamma_regularizer\": tf.keras.regularizers.serialize(\n",
    "                self.gamma_regularizer\n",
    "            ),\n",
    "            \"beta_constraint\": tf.keras.constraints.serialize(self.beta_constraint),\n",
    "            \"gamma_constraint\": tf.keras.constraints.serialize(self.gamma_constraint),\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def _reshape_into_groups(self, inputs, input_shape, tensor_input_shape):\n",
    "\n",
    "        group_shape = [tensor_input_shape[i] for i in range(len(input_shape))]\n",
    "        group_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        group_shape.insert(self.axis, self.groups)\n",
    "        group_shape = tf.stack(group_shape)\n",
    "        reshaped_inputs = tf.reshape(inputs, group_shape)\n",
    "        return reshaped_inputs, group_shape\n",
    "\n",
    "    def _apply_normalization(self, reshaped_inputs, input_shape):\n",
    "\n",
    "        group_shape = tf.keras.backend.int_shape(reshaped_inputs)\n",
    "        group_reduction_axes = list(range(1, len(group_shape)))\n",
    "        axis = -2 if self.axis == -1 else self.axis - 1\n",
    "        group_reduction_axes.pop(axis)\n",
    "\n",
    "        mean, variance = tf.nn.moments(\n",
    "            reshaped_inputs, group_reduction_axes, keepdims=True\n",
    "        )\n",
    "\n",
    "        gamma, beta = self._get_reshaped_weights(input_shape)\n",
    "        normalized_inputs = tf.nn.batch_normalization(\n",
    "            reshaped_inputs,\n",
    "            mean=mean,\n",
    "            variance=variance,\n",
    "            scale=gamma,\n",
    "            offset=beta,\n",
    "            variance_epsilon=self.epsilon,\n",
    "        )\n",
    "        return normalized_inputs\n",
    "\n",
    "    def _get_reshaped_weights(self, input_shape):\n",
    "        broadcast_shape = self._create_broadcast_shape(input_shape)\n",
    "        gamma = None\n",
    "        beta = None\n",
    "        if self.scale:\n",
    "            gamma = tf.reshape(self.gamma, broadcast_shape)\n",
    "\n",
    "        if self.center:\n",
    "            beta = tf.reshape(self.beta, broadcast_shape)\n",
    "        return gamma, beta\n",
    "\n",
    "    def _check_if_input_shape_is_none(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim is None:\n",
    "            raise ValueError(\n",
    "                \"Axis \" + str(self.axis) + \" of \"\n",
    "                                           \"input tensor should have a defined dimension \"\n",
    "                                           \"but the layer received an input with shape \" + str(input_shape) + \".\"\n",
    "            )\n",
    "\n",
    "    def _set_number_of_groups_for_instance_norm(self, input_shape):\n",
    "        dim = input_shape[self.axis]\n",
    "\n",
    "        if self.groups == -1:\n",
    "            self.groups = dim\n",
    "\n",
    "    def _check_size_of_dimensions(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        if dim < self.groups:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") cannot be \"\n",
    "                                                          \"more than the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "        if dim % self.groups != 0:\n",
    "            raise ValueError(\n",
    "                \"Number of groups (\" + str(self.groups) + \") must be a \"\n",
    "                                                          \"multiple of the number of channels (\" + str(dim) + \").\"\n",
    "            )\n",
    "\n",
    "    def _check_axis(self):\n",
    "\n",
    "        if self.axis == 0:\n",
    "            raise ValueError(\n",
    "                \"You are trying to normalize your batch axis. Do you want to \"\n",
    "                \"use tf.layer.batch_normalization instead\"\n",
    "            )\n",
    "\n",
    "    def _create_input_spec(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            ndim=len(input_shape), axes={self.axis: dim}\n",
    "        )\n",
    "\n",
    "    def _add_gamma_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.scale:\n",
    "            self.gamma = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"gamma\",\n",
    "                initializer=self.gamma_initializer,\n",
    "                regularizer=self.gamma_regularizer,\n",
    "                constraint=self.gamma_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.gamma = None\n",
    "\n",
    "    def _add_beta_weight(self, input_shape):\n",
    "\n",
    "        dim = input_shape[self.axis]\n",
    "        shape = (dim,)\n",
    "\n",
    "        if self.center:\n",
    "            self.beta = self.add_weight(\n",
    "                shape=shape,\n",
    "                name=\"beta\",\n",
    "                initializer=self.beta_initializer,\n",
    "                regularizer=self.beta_regularizer,\n",
    "                constraint=self.beta_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.beta = None\n",
    "\n",
    "    def _create_broadcast_shape(self, input_shape):\n",
    "        broadcast_shape = [1] * len(input_shape)\n",
    "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
    "        broadcast_shape.insert(self.axis, self.groups)\n",
    "        return broadcast_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformBlock(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, features,\n",
    "                 norm_type,\n",
    "                 momentum=0.9,\n",
    "                 virtual_batch_size=None,\n",
    "                 groups=2,\n",
    "                 block_name='',\n",
    "                 **kwargs):\n",
    "        super(TransformBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.features = features\n",
    "        self.norm_type = norm_type\n",
    "        self.momentum = momentum\n",
    "        self.groups = groups\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "\n",
    "        self.transform = tf.keras.layers.Dense(self.features, use_bias=False, name=f'transformblock_dense_{block_name}')\n",
    "\n",
    "        if norm_type == 'batch':\n",
    "            self.bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=momentum,\n",
    "                                                         virtual_batch_size=virtual_batch_size,\n",
    "                                                         name=f'transformblock_bn_{block_name}')\n",
    "        \n",
    "        elif norm_type == \"ghost\":\n",
    "            self.bn = GhostBatchNormalization(virtual_divider=32, momentum=0.9)\n",
    "\n",
    "        else:\n",
    "            self.bn = GroupNormalization(axis=-1, groups=self.groups, name=f'transformblock_gn_{block_name}')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.transform(inputs)\n",
    "        x = self.bn(x, training=training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TabNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_features=None,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=2,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'batch' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNet, self).__init__(**kwargs)\n",
    "        if random_state is not None:\n",
    "            tf.keras.utils.set_random_seed(random_state)\n",
    "\n",
    "        # Input checks\n",
    "        if feature_columns is not None:\n",
    "            if type(feature_columns) not in (list, tuple):\n",
    "                raise ValueError(\"`feature_columns` must be a list or a tuple.\")\n",
    "\n",
    "            if len(feature_columns) == 0:\n",
    "                raise ValueError(\"`feature_columns` must be contain at least 1 tf.feature_column !\")\n",
    "\n",
    "            if num_features is None:\n",
    "                num_features = len(feature_columns)\n",
    "            else:\n",
    "                num_features = int(num_features)\n",
    "\n",
    "        else:\n",
    "            if num_features is None:\n",
    "                raise ValueError(\"If `feature_columns` is None, then `num_features` cannot be None.\")\n",
    "\n",
    "        if num_decision_steps < 1:\n",
    "            raise ValueError(\"Num decision steps must be greater than 0.\")\n",
    "\n",
    "        if feature_dim <= output_dim:\n",
    "            raise ValueError(\"To compute `features_for_coef`, feature_dim must be larger than output dim\")\n",
    "\n",
    "        feature_dim = int(feature_dim)\n",
    "        output_dim = int(output_dim)\n",
    "        num_decision_steps = int(num_decision_steps)\n",
    "        relaxation_factor = float(relaxation_factor)\n",
    "        sparsity_coefficient = float(sparsity_coefficient)\n",
    "        batch_momentum = float(batch_momentum)\n",
    "        num_groups = max(1, int(num_groups))\n",
    "        epsilon = float(epsilon)\n",
    "\n",
    "        if relaxation_factor < 0.:\n",
    "            raise ValueError(\"`relaxation_factor` cannot be negative !\")\n",
    "\n",
    "        if sparsity_coefficient < 0.:\n",
    "            raise ValueError(\"`sparsity_coefficient` cannot be negative !\")\n",
    "\n",
    "        if virtual_batch_size is not None:\n",
    "            virtual_batch_size = int(virtual_batch_size)\n",
    "\n",
    "        if norm_type not in ['batch', 'ghost', 'group']:\n",
    "            raise ValueError(\"`norm_type` must be either `batch` or `group`\")\n",
    "\n",
    "        self.feature_columns = feature_columns\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.sparsity_coefficient = sparsity_coefficient\n",
    "        self.norm_type = norm_type\n",
    "        self.batch_momentum = batch_momentum\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        self.num_groups = num_groups\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if num_decision_steps > 1:\n",
    "            features_for_coeff = feature_dim - output_dim\n",
    "            print(f\"[TabNet]: {features_for_coeff} features will be used for decision steps.\")\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            self.input_features = tf.keras.layers.DenseFeatures(feature_columns, trainable=True)\n",
    "\n",
    "            if self.norm_type == 'batch':\n",
    "                self.input_bn = tf.keras.layers.BatchNormalization(axis=-1, momentum=batch_momentum, name='input_bn')\n",
    "            else:\n",
    "                self.input_bn = GroupNormalization(axis=-1, groups=self.num_groups, name='input_gn')\n",
    "\n",
    "        else:\n",
    "            self.input_features = None\n",
    "            self.input_bn = None\n",
    "\n",
    "        self.transform_f1 = TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n",
    "                                           block_name='f1')\n",
    "\n",
    "        self.transform_f2 = TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                                           self.batch_momentum, self.virtual_batch_size, self.num_groups,\n",
    "                                           block_name='f2')\n",
    "\n",
    "        self.transform_f3_list = [\n",
    "            TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f3_{i}')\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_f4_list = [\n",
    "            TransformBlock(2 * self.feature_dim, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'f4_{i}')\n",
    "            for i in range(self.num_decision_steps)\n",
    "        ]\n",
    "\n",
    "        self.transform_coef_list = [\n",
    "            TransformBlock(self.num_features, self.norm_type,\n",
    "                           self.batch_momentum, self.virtual_batch_size, self.num_groups, block_name=f'coef_{i}')\n",
    "            for i in range(self.num_decision_steps - 1)\n",
    "        ]\n",
    "\n",
    "        self._step_feature_selection_masks = None\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if self.input_features is not None:\n",
    "            features = self.input_features(inputs)\n",
    "            features = self.input_bn(features, training=training)\n",
    "\n",
    "        else:\n",
    "            features = inputs\n",
    "\n",
    "        batch_size = tf.shape(features)[0]\n",
    "        self._step_feature_selection_masks = []\n",
    "        self._step_aggregate_feature_selection_mask = None\n",
    "\n",
    "        # Initializes decision-step dependent variables.\n",
    "        output_aggregated = tf.zeros([batch_size, self.output_dim])\n",
    "        masked_features = features\n",
    "        mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        aggregated_mask_values = tf.zeros([batch_size, self.num_features])\n",
    "        complementary_aggregated_mask_values = tf.ones(\n",
    "            [batch_size, self.num_features])\n",
    "\n",
    "        total_entropy = 0.0\n",
    "        entropy_loss = 0.\n",
    "\n",
    "        for ni in range(self.num_decision_steps):\n",
    "            # Feature transformer with two shared and two decision step dependent\n",
    "            # blocks is used below.=\n",
    "            transform_f1 = self.transform_f1(masked_features, training=training)\n",
    "            transform_f1 = glu(transform_f1, self.feature_dim)\n",
    "\n",
    "            transform_f2 = self.transform_f2(transform_f1, training=training)\n",
    "            transform_f2 = (glu(transform_f2, self.feature_dim) +\n",
    "                            transform_f1) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f3 = self.transform_f3_list[ni](transform_f2, training=training)\n",
    "            transform_f3 = (glu(transform_f3, self.feature_dim) +\n",
    "                            transform_f2) * tf.math.sqrt(0.5)\n",
    "\n",
    "            transform_f4 = self.transform_f4_list[ni](transform_f3, training=training)\n",
    "            transform_f4 = (glu(transform_f4, self.feature_dim) +\n",
    "                            transform_f3) * tf.math.sqrt(0.5)\n",
    "\n",
    "            if (ni > 0 or self.num_decision_steps == 1):\n",
    "                decision_out = tf.nn.relu(transform_f4[:, :self.output_dim])\n",
    "\n",
    "                # Decision aggregation.\n",
    "                output_aggregated += decision_out\n",
    "\n",
    "                # Aggregated masks are used for visualization of the\n",
    "                # feature importance attributes.\n",
    "                scale_agg = tf.reduce_sum(decision_out, axis=1, keepdims=True)\n",
    "\n",
    "                if self.num_decision_steps > 1:\n",
    "                    scale_agg = scale_agg / tf.cast(self.num_decision_steps - 1, tf.float32)\n",
    "\n",
    "                aggregated_mask_values += mask_values * scale_agg\n",
    "\n",
    "            features_for_coef = transform_f4[:, self.output_dim:]\n",
    "\n",
    "            if ni < (self.num_decision_steps - 1):\n",
    "                # Determines the feature masks via linear and nonlinear\n",
    "                # transformations, taking into account of aggregated feature use.\n",
    "                mask_values = self.transform_coef_list[ni](features_for_coef, training=training)\n",
    "                mask_values *= complementary_aggregated_mask_values\n",
    "                mask_values = sparsemax(mask_values, axis=-1)\n",
    "\n",
    "                # Relaxation factor controls the amount of reuse of features between\n",
    "                # different decision blocks and updated with the values of\n",
    "                # coefficients.\n",
    "                complementary_aggregated_mask_values *= (\n",
    "                        self.relaxation_factor - mask_values)\n",
    "\n",
    "                # Entropy is used to penalize the amount of sparsity in feature\n",
    "                # selection.\n",
    "                total_entropy += tf.reduce_mean(\n",
    "                    tf.reduce_sum(\n",
    "                        -mask_values * tf.math.log(mask_values + self.epsilon), axis=1)) / (\n",
    "                                     tf.cast(self.num_decision_steps - 1, tf.float32))\n",
    "\n",
    "                # Add entropy loss\n",
    "                entropy_loss = total_entropy\n",
    "\n",
    "                # Feature selection.\n",
    "                masked_features = tf.multiply(mask_values, features)\n",
    "\n",
    "                # Visualization of the feature selection mask at decision step ni\n",
    "                # tf.summary.image(\n",
    "                #     \"Mask for step\" + str(ni),\n",
    "                #     tf.expand_dims(tf.expand_dims(mask_values, 0), 3),\n",
    "                #     max_outputs=1)\n",
    "                mask_at_step_i = tf.expand_dims(tf.expand_dims(mask_values, 0), 3)\n",
    "                self._step_feature_selection_masks.append(mask_at_step_i)\n",
    "\n",
    "            else:\n",
    "                # This branch is needed for correct compilation by tf.autograph\n",
    "                entropy_loss = 0.\n",
    "\n",
    "        # Adds the loss automatically\n",
    "        self.add_loss(self.sparsity_coefficient * entropy_loss)\n",
    "\n",
    "        # Visualization of the aggregated feature importances\n",
    "        # tf.summary.image(\n",
    "        #     \"Aggregated mask\",\n",
    "        #     tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3),\n",
    "        #     max_outputs=1)\n",
    "\n",
    "        agg_mask = tf.expand_dims(tf.expand_dims(aggregated_mask_values, 0), 3)\n",
    "        self._step_aggregate_feature_selection_mask = agg_mask\n",
    "\n",
    "        return output_aggregated\n",
    "\n",
    "    @property\n",
    "    def feature_selection_masks(self):\n",
    "        return self._step_feature_selection_masks\n",
    "\n",
    "    @property\n",
    "    def aggregate_feature_selection_mask(self):\n",
    "        return self._step_aggregate_feature_selection_mask\n",
    "\n",
    "\n",
    "class TabNetClassifier(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 num_classes,\n",
    "                 num_features=None,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=1,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_classes: Number of classes.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetClassifier, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.tabnet = TabNet(feature_columns=feature_columns,\n",
    "                             num_features=num_features,\n",
    "                             feature_dim=feature_dim,\n",
    "                             output_dim=output_dim,\n",
    "                             num_decision_steps=num_decision_steps,\n",
    "                             relaxation_factor=relaxation_factor,\n",
    "                             sparsity_coefficient=sparsity_coefficient,\n",
    "                             norm_type=norm_type,\n",
    "                             batch_momentum=batch_momentum,\n",
    "                             virtual_batch_size=virtual_batch_size,\n",
    "                             num_groups=num_groups,\n",
    "                             epsilon=epsilon,\n",
    "                             random_state=random_state,\n",
    "                             **kwargs)\n",
    "\n",
    "        self.clf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False, name='classifier')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.clf(self.activations)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)\n",
    "\n",
    "\n",
    "class TabNetRegressor(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, feature_columns,\n",
    "                 num_regressors,\n",
    "                 num_features=None,\n",
    "                 feature_dim=64,\n",
    "                 output_dim=64,\n",
    "                 num_decision_steps=5,\n",
    "                 relaxation_factor=1.5,\n",
    "                 sparsity_coefficient=1e-5,\n",
    "                 norm_type='group',\n",
    "                 batch_momentum=0.98,\n",
    "                 virtual_batch_size=None,\n",
    "                 num_groups=1,\n",
    "                 epsilon=1e-5,\n",
    "                 random_state = None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Tensorflow 2.0 implementation of [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)\n",
    "\n",
    "        # Hyper Parameter Tuning (Excerpt from the paper)\n",
    "        We consider datasets ranging from ∼10K to ∼10M training points, with varying degrees of fitting\n",
    "        difficulty. TabNet obtains high performance for all with a few general principles on hyperparameter\n",
    "        selection:\n",
    "\n",
    "            - Most datasets yield the best results for Nsteps ∈ [3, 10]. Typically, larger datasets and\n",
    "            more complex tasks require a larger Nsteps. A very high value of Nsteps may suffer from\n",
    "            overfitting and yield poor generalization.\n",
    "\n",
    "            - Adjustment of the values of Nd and Na is the most efficient way of obtaining a trade-off\n",
    "            between performance and complexity. Nd = Na is a reasonable choice for most datasets. A\n",
    "            very high value of Nd and Na may suffer from overfitting and yield poor generalization.\n",
    "\n",
    "            - An optimal choice of γ can have a major role on the overall performance. Typically a larger\n",
    "            Nsteps value favors for a larger γ.\n",
    "\n",
    "            - A large batch size is beneficial for performance - if the memory constraints permit, as large\n",
    "            as 1-10 % of the total training dataset size is suggested. The virtual batch size is typically\n",
    "            much smaller than the batch size.\n",
    "\n",
    "            - Initially large learning rate is important, which should be gradually decayed until convergence.\n",
    "\n",
    "        Args:\n",
    "            feature_columns: The Tensorflow feature columns for the dataset.\n",
    "            num_regressors: Number of regression variables.\n",
    "            feature_dim (N_a): Dimensionality of the hidden representation in feature\n",
    "                transformation block. Each layer first maps the representation to a\n",
    "                2*feature_dim-dimensional output and half of it is used to determine the\n",
    "                nonlinearity of the GLU activation where the other half is used as an\n",
    "                input to GLU, and eventually feature_dim-dimensional output is\n",
    "                transferred to the next layer.\n",
    "            output_dim (N_d): Dimensionality of the outputs of each decision step, which is\n",
    "                later mapped to the final classification or regression output.\n",
    "            num_features: The number of input features (i.e the number of columns for\n",
    "                tabular data assuming each feature is represented with 1 dimension).\n",
    "            num_decision_steps(N_steps): Number of sequential decision steps.\n",
    "            relaxation_factor (gamma): Relaxation factor that promotes the reuse of each\n",
    "                feature at different decision steps. When it is 1, a feature is enforced\n",
    "                to be used only at one decision step and as it increases, more\n",
    "                flexibility is provided to use a feature at multiple decision steps.\n",
    "            sparsity_coefficient (lambda_sparse): Strength of the sparsity regularization.\n",
    "                Sparsity may provide a favorable inductive bias for convergence to\n",
    "                higher accuracy for some datasets where most of the input features are redundant.\n",
    "            norm_type: Type of normalization to perform for the model. Can be either\n",
    "                'group' or 'group'. 'group' is the default.\n",
    "            batch_momentum: Momentum in ghost batch normalization.\n",
    "            virtual_batch_size: Virtual batch size in ghost batch normalization. The\n",
    "                overall batch size should be an integer multiple of virtual_batch_size.\n",
    "            num_groups: Number of groups used for group normalization.\n",
    "            epsilon: A small number for numerical stability of the entropy calculations.\n",
    "        \"\"\"\n",
    "        super(TabNetRegressor, self).__init__(**kwargs)\n",
    "\n",
    "        self.num_regressors = num_regressors\n",
    "\n",
    "        self.tabnet = TabNet(feature_columns=feature_columns,\n",
    "                             num_features=num_features,\n",
    "                             feature_dim=feature_dim,\n",
    "                             output_dim=output_dim,\n",
    "                             num_decision_steps=num_decision_steps,\n",
    "                             relaxation_factor=relaxation_factor,\n",
    "                             sparsity_coefficient=sparsity_coefficient,\n",
    "                             norm_type=norm_type,\n",
    "                             batch_momentum=batch_momentum,\n",
    "                             virtual_batch_size=virtual_batch_size,\n",
    "                             num_groups=num_groups,\n",
    "                             epsilon=epsilon,\n",
    "                             random_state=random_state,\n",
    "                             **kwargs)\n",
    "\n",
    "        self.regressor = tf.keras.layers.Dense(num_regressors, use_bias=False, name='regressor')\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        self.activations = self.tabnet(inputs, training=training)\n",
    "        out = self.regressor(self.activations)\n",
    "        return out\n",
    "\n",
    "    def summary(self, *super_args, **super_kwargs):\n",
    "        super().summary(*super_args, **super_kwargs)\n",
    "        self.tabnet.summary(*super_args, **super_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TabNet]: 4 features will be used for decision steps.\n"
     ]
    }
   ],
   "source": [
    "# Group Norm does better for small datasets\n",
    "model = TabNetClassifier(feature_columns, num_classes=3,\n",
    "                        feature_dim=8, output_dim=4,\n",
    "                        num_decision_steps=4, relaxation_factor=1.0,\n",
    "                        sparsity_coefficient=1e-5, batch_momentum=0.98,\n",
    "                        virtual_batch_size=None, norm_type='ghost',\n",
    "                        num_groups=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=100, decay_rate=0.9, staircase=False)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(lr)\n",
    "model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 12:35:33.102440: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method BatchNormInferenceWeighting.call of <__main__.BatchNormInferenceWeighting object at 0x2960cc9d0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method BatchNormInferenceWeighting.call of <__main__.BatchNormInferenceWeighting object at 0x2960cc9d0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method BatchNormInferenceWeighting.call of <__main__.BatchNormInferenceWeighting object at 0x2960cc9d0>> and will run it as-is.\n",
      "Cause: mangled names are not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split' defined at (most recent call last):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4148100193.py\", line 1, in <module>\n      model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=1)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 425, in call\n      self.activations = self.tabnet(inputs, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 236, in call\n      for ni in range(self.num_decision_steps):\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 239, in call\n      transform_f1 = self.transform_f1(masked_features, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 33, in call\n      x = self.bn(x, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 11, in call\n      if training:\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 12, in call\n      chunks = tf.split(x, self.virtual_divider)\nNode: 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split'\nDetected at node 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split' defined at (most recent call last):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4148100193.py\", line 1, in <module>\n      model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=1)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 425, in call\n      self.activations = self.tabnet(inputs, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 236, in call\n      for ni in range(self.num_decision_steps):\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 239, in call\n      transform_f1 = self.transform_f1(masked_features, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 33, in call\n      x = self.bn(x, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 11, in call\n      if training:\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 12, in call\n      chunks = tf.split(x, self.virtual_divider)\nNode: 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 50) and num_split 32\n\t [[{{node tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split}}]]\n\t [[tab_net_classifier/tab_net/transform_block_6/ghost_batch_normalization_6/batch_norm_inference_weighting_6/Sqrt_9/_5560]]\n  (1) INVALID_ARGUMENT:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 50) and num_split 32\n\t [[{{node tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_169683]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mds_test, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split' defined at (most recent call last):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4148100193.py\", line 1, in <module>\n      model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=1)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 425, in call\n      self.activations = self.tabnet(inputs, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 236, in call\n      for ni in range(self.num_decision_steps):\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 239, in call\n      transform_f1 = self.transform_f1(masked_features, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 33, in call\n      x = self.bn(x, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 11, in call\n      if training:\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 12, in call\n      chunks = tf.split(x, self.virtual_divider)\nNode: 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split'\nDetected at node 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split' defined at (most recent call last):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/base_events.py\", line 1906, in _run_once\n      handle._run()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4148100193.py\", line 1, in <module>\n      model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=1)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 425, in call\n      self.activations = self.tabnet(inputs, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 236, in call\n      for ni in range(self.num_decision_steps):\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 239, in call\n      transform_f1 = self.transform_f1(masked_features, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/2617227599.py\", line 33, in call\n      x = self.bn(x, training=training)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/dkmoon/anaconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 11, in call\n      if training:\n    File \"/var/folders/mv/w0_84cdx22j8qjdy0s7gs08r0000gn/T/ipykernel_27820/4021591316.py\", line 12, in call\n      chunks = tf.split(x, self.virtual_divider)\nNode: 'tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split'\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 50) and num_split 32\n\t [[{{node tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split}}]]\n\t [[tab_net_classifier/tab_net/transform_block_6/ghost_batch_normalization_6/batch_norm_inference_weighting_6/Sqrt_9/_5560]]\n  (1) INVALID_ARGUMENT:  Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 50) and num_split 32\n\t [[{{node tab_net_classifier/tab_net/transform_block/ghost_batch_normalization/split}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_169683]"
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=100, validation_data=ds_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_net_classifier\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tab_net (TabNet)            multiple                  1616      \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  12        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,628\n",
      "Trainable params: 1,628\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"tab_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_features (DenseFeatur  multiple                 0         \n",
      " es)                                                             \n",
      "                                                                 \n",
      " input_gn (GroupNormalizatio  multiple                 8         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " transform_block (TransformB  multiple                 96        \n",
      " lock)                                                           \n",
      "                                                                 \n",
      " transform_block_1 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_2 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_3 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_4 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_5 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_6 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_7 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_8 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_9 (Transfor  multiple                 160       \n",
      " mBlock)                                                         \n",
      "                                                                 \n",
      " transform_block_10 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      " transform_block_11 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      " transform_block_12 (Transfo  multiple                 24        \n",
      " rmBlock)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,616\n",
      "Trainable params: 1,616\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 21:42:57.720408: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "pred = model.predict(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.x",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
